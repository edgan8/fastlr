{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"~/data/avazu/train\",\n",
    "    nrows=1000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"click\"\n",
    "CAT_COLS = [\n",
    "    \"C1\", \"banner_pos\", \n",
    "    \"site_category\", \"app_category\", \n",
    "    \"device_type\", \"device_conn_type\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = pd.get_dummies(df[CAT_COLS], columns=CAT_COLS)\n",
    "df_final = pd.concat([\n",
    "    df[target], df_enc\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = df_enc.values\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 59 epochs took 74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.42532564432012976\n"
     ]
    }
   ],
   "source": [
    "lm = sklearn.linear_model.LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    verbose=1,\n",
    "    max_iter=100,\n",
    ")\n",
    "lm.fit(Xs, y)\n",
    "yh = lm.predict_proba(Xs)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.738578, T: 1000000, Avg. loss: 0.434745\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.17, NNZs: 66, Bias: -1.727277, T: 2000000, Avg. loss: 0.434426\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.22, NNZs: 66, Bias: -1.726153, T: 3000000, Avg. loss: 0.434416\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.18, NNZs: 66, Bias: -1.738806, T: 4000000, Avg. loss: 0.434413\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.19, NNZs: 66, Bias: -1.741730, T: 5000000, Avg. loss: 0.434401\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.719802, T: 6000000, Avg. loss: 0.434430\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.719315, T: 7000000, Avg. loss: 0.434280\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.714909, T: 8000000, Avg. loss: 0.434277\n",
      "Total training time: 3.99 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.721505, T: 9000000, Avg. loss: 0.434283\n",
      "Total training time: 4.48 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.715129, T: 10000000, Avg. loss: 0.434272\n",
      "Total training time: 4.98 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.723757, T: 11000000, Avg. loss: 0.434285\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.726344, T: 12000000, Avg. loss: 0.434223\n",
      "Total training time: 5.97 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.724068, T: 13000000, Avg. loss: 0.434249\n",
      "Total training time: 6.46 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.722193, T: 14000000, Avg. loss: 0.434258\n",
      "Total training time: 6.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.723768, T: 15000000, Avg. loss: 0.434238\n",
      "Total training time: 7.45 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.723423, T: 16000000, Avg. loss: 0.434248\n",
      "Total training time: 7.94 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.722763, T: 17000000, Avg. loss: 0.434264\n",
      "Total training time: 8.44 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.722345, T: 18000000, Avg. loss: 0.434252\n",
      "Total training time: 8.93 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.722884, T: 19000000, Avg. loss: 0.434244\n",
      "Total training time: 9.42 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.721261, T: 20000000, Avg. loss: 0.434246\n",
      "Total training time: 9.91 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4342363321805997\n"
     ]
    }
   ],
   "source": [
    "lm = sklearn.linear_model.SGDClassifier(\n",
    "    loss = \"log\",\n",
    "    l1_ratio=0,\n",
    "    alpha=.1,\n",
    "    learning_rate=\"adaptive\",\n",
    "    eta0=.001,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    max_iter=20\n",
    ")\n",
    "lm.fit(Xs, y)\n",
    "yh = lm.predict_proba(Xs)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc = nn.Linear(k, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = LinearModel(66)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.from_numpy(Xs.astype(np.float32))\n",
    "yt = torch.from_numpy(y)\n",
    "\n",
    "train_data = utils.TensorDataset(xt,yt) # create your datset\n",
    "train_loader = utils.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Loss: 415.6251060664654\n",
      "Running Loss: 415.62961107492447\n",
      "Running Loss: 415.6205635666847\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(\"Running Loss: {}\".format(running_loss))\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.42537413860317924\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    yh = F.softmax(net(xt),dim=1)\n",
    "    score = sklearn.metrics.log_loss(y, yh)\n",
    "    print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
