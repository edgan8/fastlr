{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\n",
    "#     \"~/data/avazu/train\",\n",
    "#     nrows=10000000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_feather(\"~/data/avazu/train_10M.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"~/data/avazu/train_10M.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"click\"\n",
    "CAT_COLS = [\n",
    "    \"C1\", \"banner_pos\", \n",
    "    \"site_category\", \"app_category\", \n",
    "    \"device_type\", \"device_conn_type\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = pd.get_dummies(df[CAT_COLS], columns=CAT_COLS)\n",
    "df_final = pd.concat([\n",
    "    df[target], df_enc\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 4_000_000\n",
    "np.random.seed(0)\n",
    "r_order = np.random.permutation(nrows)\n",
    "Xs = df_enc.values[:nrows][r_order]\n",
    "y = df[target].values[:nrows][r_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = np.concatenate([\n",
    "    np.repeat(1, repeats=Xs.shape[0]).reshape(-1,1),\n",
    "    Xs\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xs, theta):\n",
    "    z = np.inner(Xs,theta)\n",
    "    h = scipy.special.expit(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(X_b, y_b, theta):\n",
    "    z = np.inner(X_b,theta)\n",
    "    h = scipy.special.expit(z)\n",
    "    grd = np.inner(X_b.T, h-y_b)/y_b.shape[0]\n",
    "    return grd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 1.4434258937835693\n",
      "Log Loss: 0.4515423413330194\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "batch_size = 512\n",
    "\n",
    "eps = 1e-8\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "t1 = time.time()\n",
    "theta = np.zeros(shape=(Xc.shape[1]))\n",
    "mt = np.zeros(shape=(Xc.shape[1]))\n",
    "vt = np.zeros(shape=(Xc.shape[1]))\n",
    "b1t = b1\n",
    "b2t = b2\n",
    "\n",
    "n_rows = len(Xc)\n",
    "start_idx = 0\n",
    "t = 1\n",
    "for batch_idx in range(n_rows // batch_size):\n",
    "    X_b = Xc[start_idx:start_idx+batch_size]\n",
    "    y_b = y[start_idx:start_idx+batch_size]\n",
    "    cur_grad = calc_grad(X_b, y_b, theta)\n",
    "    mt = b1*mt + (1-b1)*cur_grad\n",
    "    vt = b2*vt + (1-b2)*(cur_grad * cur_grad)\n",
    "    at = (lr/math.sqrt(t))*np.sqrt(1-b2t)/(1-b1t)\n",
    "#     at = lr * np.sqrt(1-b2t)/(1-b1t)\n",
    "    theta -= at*mt/(np.sqrt(vt)+eps)\n",
    "\n",
    "    start_idx += batch_size\n",
    "    b1t *= b1\n",
    "    b2t *= b2\n",
    "    t += 1\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Total Time: {}\".format(t2-t1))\n",
    "yh = predict(Xc, theta)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 0.3488016128540039\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(shape=(Xc.shape[1]))\n",
    "lr = 0.001\n",
    "\n",
    "batch_size = 128\n",
    "n_rows = len(Xc)\n",
    "start_idx = 0\n",
    "\n",
    "t1 = time.time()\n",
    "for batch_idx in range(n_rows // batch_size):\n",
    "    X_b = Xc[start_idx:start_idx+batch_size]\n",
    "    y_b = y[start_idx:start_idx+batch_size]\n",
    "    cur_grad = calc_grad(X_b, y_b, theta)\n",
    "    theta -= lr*cur_grad\n",
    "    start_idx += batch_size\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"Total Time: {}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4380691840129385\n"
     ]
    }
   ],
   "source": [
    "yh = predict(Xc, theta)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, f_name):\n",
    "    with open(f_name, \"w\") as f:\n",
    "        for row in tqdm(df.itertuples()):\n",
    "            label = getattr(row, target)\n",
    "            if label == 0:\n",
    "                lval = -1\n",
    "            else:\n",
    "                lval = 1\n",
    "            cat_vals = [\n",
    "                \"{}={}\".format(cat_name, getattr(row, cat_name))\n",
    "                for cat_name in CAT_COLS\n",
    "            ]\n",
    "            new_line = \"{} | {}\\n\".format(lval, \" \".join(cat_vals))\n",
    "            f.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000000it [00:07, 129191.29it/s]\n"
     ]
    }
   ],
   "source": [
    "process_df(df, \"vw.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/edgan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4518602557622661\n"
     ]
    }
   ],
   "source": [
    "lm = sklearn.linear_model.LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    verbose=1,\n",
    "    max_iter=30,\n",
    "    n_jobs=1,\n",
    ")\n",
    "lm.fit(Xs, y)\n",
    "yh = lm.predict_proba(Xs)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.720659, T: 1000000, Avg. loss: 0.434751\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.730077, T: 2000000, Avg. loss: 0.434397\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.708292, T: 3000000, Avg. loss: 0.434396\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.24, NNZs: 66, Bias: -1.721131, T: 4000000, Avg. loss: 0.434430\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.24, NNZs: 66, Bias: -1.718980, T: 5000000, Avg. loss: 0.434426\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.729255, T: 6000000, Avg. loss: 0.434420\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.712717, T: 7000000, Avg. loss: 0.434274\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.715451, T: 8000000, Avg. loss: 0.434283\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.727475, T: 9000000, Avg. loss: 0.434271\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.725146, T: 10000000, Avg. loss: 0.434271\n",
      "Total training time: 5.52 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.715831, T: 11000000, Avg. loss: 0.434292\n",
      "Total training time: 6.07 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.723437, T: 12000000, Avg. loss: 0.434244\n",
      "Total training time: 6.63 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.719775, T: 13000000, Avg. loss: 0.434246\n",
      "Total training time: 7.18 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.721109, T: 14000000, Avg. loss: 0.434252\n",
      "Total training time: 7.74 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.21, NNZs: 66, Bias: -1.722824, T: 15000000, Avg. loss: 0.434257\n",
      "Total training time: 8.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.720959, T: 16000000, Avg. loss: 0.434244\n",
      "Total training time: 8.85 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.720963, T: 17000000, Avg. loss: 0.434249\n",
      "Total training time: 9.40 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.722896, T: 18000000, Avg. loss: 0.434246\n",
      "Total training time: 9.96 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.722992, T: 19000000, Avg. loss: 0.434237\n",
      "Total training time: 10.51 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.20, NNZs: 66, Bias: -1.721755, T: 20000000, Avg. loss: 0.434254\n",
      "Total training time: 11.06 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4342404762471195\n"
     ]
    }
   ],
   "source": [
    "lm = sklearn.linear_model.SGDClassifier(\n",
    "    loss = \"log\",\n",
    "    l1_ratio=0,\n",
    "    alpha=.1,\n",
    "    learning_rate=\"adaptive\",\n",
    "    eta0=.001,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    max_iter=20,\n",
    "    n_jobs=4,\n",
    ")\n",
    "lm.fit(Xs, y)\n",
    "yh = lm.predict_proba(Xs)\n",
    "score = sklearn.metrics.log_loss(y, yh)\n",
    "print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc = nn.Linear(k, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = LinearModel(66)\n",
    "# net = nn.Linear(66, 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.from_numpy(Xs.astype(np.float32))\n",
    "yt = torch.from_numpy(y)\n",
    "\n",
    "train_data = utils.TensorDataset(xt,yt) # create your datset\n",
    "train_loader = utils.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Loss: 1682.9635597467422\n",
      "Finished Training in :14.4614098072052\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(\"Running Loss: {}\".format(running_loss))\n",
    "end_time = time.time()\n",
    "print('Finished Training in :{}'.format((end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.4254369985373933\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    yh = F.softmax(net(xt),dim=1)\n",
    "    score = sklearn.metrics.log_loss(y, yh)\n",
    "    print(\"Log Loss: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
